{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://osloyi5le.bkt.clouddn.com/ML%E6%A1%88%E4%BE%8Bbanner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用房价预测经典案例练习回归树建模和随机森林建模流程\n",
    "作者：寒小阳、助教：haoshuai714"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例背景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "描述案例背景：\n",
    "\n",
    "1.本案例的目的是让大家更加了解随机森林建模的过程；\n",
    "\n",
    "2.本案例采用经典的房屋价格数据集来进行学习；\n",
    "\n",
    "3.本案例将回归树训练的结果和随机森林回归树的结果进行了对比；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.决策树算法基本原理；\n",
    "\n",
    "2.随机森林的基本原理；\n",
    "\n",
    "3.回归树算法原理和随机森林算法原理；\n",
    "\n",
    "4.重点：最后补充随机森林回归树的sklearn中的参数含义；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码:构建随机森林回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import 相关的包\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载数据\n",
    "boston_house = load_boston()\n",
    "\n",
    "##调用数据集中的数据\n",
    "boston_feature_name = boston_house.feature_names\n",
    "boston_features = boston_house.data\n",
    "boston_target = boston_house.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看相关数据\n",
    "boston_feature_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#查看数据集的相关信息\n",
    "print(boston_house.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.32000000e-03,   1.80000000e+01,   2.31000000e+00,\n",
       "          0.00000000e+00,   5.38000000e-01,   6.57500000e+00,\n",
       "          6.52000000e+01,   4.09000000e+00,   1.00000000e+00,\n",
       "          2.96000000e+02,   1.53000000e+01,   3.96900000e+02,\n",
       "          4.98000000e+00],\n",
       "       [  2.73100000e-02,   0.00000000e+00,   7.07000000e+00,\n",
       "          0.00000000e+00,   4.69000000e-01,   6.42100000e+00,\n",
       "          7.89000000e+01,   4.96710000e+00,   2.00000000e+00,\n",
       "          2.42000000e+02,   1.78000000e+01,   3.96900000e+02,\n",
       "          9.14000000e+00],\n",
       "       [  2.72900000e-02,   0.00000000e+00,   7.07000000e+00,\n",
       "          0.00000000e+00,   4.69000000e-01,   7.18500000e+00,\n",
       "          6.11000000e+01,   4.96710000e+00,   2.00000000e+00,\n",
       "          2.42000000e+02,   1.78000000e+01,   3.92830000e+02,\n",
       "          4.03000000e+00],\n",
       "       [  3.23700000e-02,   0.00000000e+00,   2.18000000e+00,\n",
       "          0.00000000e+00,   4.58000000e-01,   6.99800000e+00,\n",
       "          4.58000000e+01,   6.06220000e+00,   3.00000000e+00,\n",
       "          2.22000000e+02,   1.87000000e+01,   3.94630000e+02,\n",
       "          2.94000000e+00],\n",
       "       [  6.90500000e-02,   0.00000000e+00,   2.18000000e+00,\n",
       "          0.00000000e+00,   4.58000000e-01,   7.14700000e+00,\n",
       "          5.42000000e+01,   6.06220000e+00,   3.00000000e+00,\n",
       "          2.22000000e+02,   1.87000000e+01,   3.96900000e+02,\n",
       "          5.33000000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#插卡数据集中的前5行\n",
    "boston_features[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,\n",
       "        18.9,  15. ,  18.9,  21.7,  20.4,  18.2,  19.9,  23.1,  17.5,\n",
       "        20.2,  18.2,  13.6,  19.6,  15.2,  14.5,  15.6,  13.9,  16.6,\n",
       "        14.8,  18.4,  21. ,  12.7,  14.5,  13.2,  13.1,  13.5,  18.9,\n",
       "        20. ,  21. ,  24.7,  30.8,  34.9,  26.6,  25.3,  24.7,  21.2,\n",
       "        19.3,  20. ,  16.6,  14.4,  19.4,  19.7,  20.5,  25. ,  23.4,\n",
       "        18.9,  35.4,  24.7,  31.6,  23.3,  19.6,  18.7,  16. ,  22.2,\n",
       "        25. ,  33. ,  23.5,  19.4,  22. ,  17.4,  20.9,  24.2,  21.7,\n",
       "        22.8,  23.4,  24.1,  21.4,  20. ,  20.8,  21.2,  20.3,  28. ,\n",
       "        23.9,  24.8,  22.9,  23.9,  26.6,  22.5,  22.2,  23.6,  28.7,\n",
       "        22.6,  22. ,  22.9,  25. ,  20.6,  28.4,  21.4,  38.7,  43.8,\n",
       "        33.2,  27.5,  26.5,  18.6,  19.3,  20.1,  19.5,  19.5,  20.4,\n",
       "        19.8,  19.4,  21.7,  22.8,  18.8,  18.7,  18.5,  18.3,  21.2,\n",
       "        19.2,  20.4,  19.3,  22. ,  20.3,  20.5,  17.3,  18.8,  21.4,\n",
       "        15.7,  16.2,  18. ,  14.3,  19.2,  19.6,  23. ,  18.4,  15.6,\n",
       "        18.1,  17.4,  17.1,  13.3,  17.8,  14. ,  14.4,  13.4,  15.6,\n",
       "        11.8,  13.8,  15.6,  14.6,  17.8,  15.4,  21.5,  19.6,  15.3,\n",
       "        19.4,  17. ,  15.6,  13.1,  41.3,  24.3,  23.3,  27. ,  50. ,\n",
       "        50. ,  50. ,  22.7,  25. ,  50. ,  23.8,  23.8,  22.3,  17.4,\n",
       "        19.1,  23.1,  23.6,  22.6,  29.4,  23.2,  24.6,  29.9,  37.2,\n",
       "        39.8,  36.2,  37.9,  32.5,  26.4,  29.6,  50. ,  32. ,  29.8,\n",
       "        34.9,  37. ,  30.5,  36.4,  31.1,  29.1,  50. ,  33.3,  30.3,\n",
       "        34.6,  34.9,  32.9,  24.1,  42.3,  48.5,  50. ,  22.6,  24.4,\n",
       "        22.5,  24.4,  20. ,  21.7,  19.3,  22.4,  28.1,  23.7,  25. ,\n",
       "        23.3,  28.7,  21.5,  23. ,  26.7,  21.7,  27.5,  30.1,  44.8,\n",
       "        50. ,  37.6,  31.6,  46.7,  31.5,  24.3,  31.7,  41.7,  48.3,\n",
       "        29. ,  24. ,  25.1,  31.5,  23.7,  23.3,  22. ,  20.1,  22.2,\n",
       "        23.7,  17.6,  18.5,  24.3,  20.5,  24.5,  26.2,  24.4,  24.8,\n",
       "        29.6,  42.8,  21.9,  20.9,  44. ,  50. ,  36. ,  30.1,  33.8,\n",
       "        43.1,  48.8,  31. ,  36.5,  22.8,  30.7,  50. ,  43.5,  20.7,\n",
       "        21.1,  25.2,  24.4,  35.2,  32.4,  32. ,  33.2,  33.1,  29.1,\n",
       "        35.1,  45.4,  35.4,  46. ,  50. ,  32.2,  22. ,  20.1,  23.2,\n",
       "        22.3,  24.8,  28.5,  37.3,  27.9,  23.9,  21.7,  28.6,  27.1,\n",
       "        20.3,  22.5,  29. ,  24.8,  22. ,  26.4,  33.1,  36.1,  28.4,\n",
       "        33.4,  28.2,  22.8,  20.3,  16.1,  22.1,  19.4,  21.6,  23.8,\n",
       "        16.2,  17.8,  19.8,  23.1,  21. ,  23.8,  23.1,  20.4,  18.5,\n",
       "        25. ,  24.6,  23. ,  22.2,  19.3,  22.6,  19.8,  17.1,  19.4,\n",
       "        22.2,  20.7,  21.1,  19.5,  18.5,  20.6,  19. ,  18.7,  32.7,\n",
       "        16.5,  23.9,  31.2,  17.5,  17.2,  23.1,  24.5,  26.6,  22.9,\n",
       "        24.1,  18.6,  30.1,  18.2,  20.6,  17.8,  21.7,  22.7,  22.6,\n",
       "        25. ,  19.9,  20.8,  16.8,  21.9,  27.5,  21.9,  23.1,  50. ,\n",
       "        50. ,  50. ,  50. ,  50. ,  13.8,  13.8,  15. ,  13.9,  13.3,\n",
       "        13.1,  10.2,  10.4,  10.9,  11.3,  12.3,   8.8,   7.2,  10.5,\n",
       "         7.4,  10.2,  11.5,  15.1,  23.2,   9.7,  13.8,  12.7,  13.1,\n",
       "        12.5,   8.5,   5. ,   6.3,   5.6,   7.2,  12.1,   8.3,   8.5,\n",
       "         5. ,  11.9,  27.9,  17.2,  27.5,  15. ,  17.2,  17.9,  16.3,\n",
       "         7. ,   7.2,   7.5,  10.4,   8.8,   8.4,  16.7,  14.2,  20.8,\n",
       "        13.4,  11.7,   8.3,  10.2,  10.9,  11. ,   9.5,  14.5,  14.1,\n",
       "        16.1,  14.3,  11.7,  13.4,   9.6,   8.7,   8.4,  12.8,  10.5,\n",
       "        17.1,  18.4,  15.4,  10.8,  11.8,  14.9,  12.6,  14.1,  13. ,\n",
       "        13.4,  15.2,  16.1,  17.8,  14.9,  14.1,  12.7,  13.5,  14.9,\n",
       "        20. ,  16.4,  17.7,  19.5,  20.2,  21.4,  19.9,  19. ,  19.1,\n",
       "        19.1,  20.1,  19.9,  19.6,  23.2,  29.8,  13.8,  13.3,  16.7,\n",
       "        12. ,  14.6,  21.4,  23. ,  23.7,  25. ,  21.8,  20.6,  21.2,\n",
       "        19.1,  20.6,  15.2,   7. ,   8.1,  13.6,  20.1,  21.8,  24.5,\n",
       "        23.1,  19.7,  18.3,  21.2,  17.5,  16.8,  22.4,  20.6,  23.9,\n",
       "        22. ,  11.9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看数据集的target\n",
    "boston_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestRegressor in module sklearn.ensemble.forest:\n",
      "\n",
      "class RandomForestRegressor(ForestRegressor)\n",
      " |  A random forest regressor.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of classifying\n",
      " |  decision trees on various sub-samples of the dataset and use averaging\n",
      " |  to improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is always the same as the original\n",
      " |  input sample size but the samples are drawn with replacement if\n",
      " |  `bootstrap=True` (default).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : integer, optional (default=10)\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"mse\")\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"mse\" for the mean squared error, which is equal to variance\n",
      " |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      " |      absolute error.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a percentage and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_depth : integer or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a percentage and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a percentage and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_split : float,\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=True)\n",
      " |      Whether bootstrap samples are used when building trees.\n",
      " |  \n",
      " |  oob_score : bool, optional (default=False)\n",
      " |      whether to use out-of-bag samples to estimate\n",
      " |      the R^2 on unseen data.\n",
      " |  \n",
      " |  n_jobs : integer, optional (default=1)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      If -1, then the number of jobs is set to the number of cores.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity of the tree building process.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeRegressor\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |  \n",
      " |  oob_prediction_ : array of shape = [n_samples]\n",
      " |      Prediction computed with out-of-bag estimate on the training set.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>>\n",
      " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      " |  ...                        random_state=0, shuffle=False)\n",
      " |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
      " |             max_features='auto', max_leaf_nodes=None,\n",
      " |             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      " |             min_samples_leaf=1, min_samples_split=2,\n",
      " |             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      " |             oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      " |  >>> print(regr.feature_importances_)\n",
      " |  [ 0.17339552  0.81594114  0.          0.01066333]\n",
      " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      " |  [-2.50699856]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor, ExtraTreesRegressor\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestRegressor\n",
      " |      ForestRegressor\n",
      " |      abc.NewBase\n",
      " |      BaseForest\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=10, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestRegressor:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |      \n",
      " |      The predicted regression target of an input sample is computed as the\n",
      " |      mean predicted regression targets of the trees in the forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      " |          Return a node indicator matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |      \n",
      " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#查看随机森林回归书的相关帮助文档\n",
    "help(RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#初始化一个随机森林回归树模型，包含15个决策树\n",
    "rgs = RandomForestRegressor(n_estimators=15)\n",
    "#训练模型\n",
    "rgs = rgs.fit(boston_features, boston_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24.81333333,  22.03333333,  33.46666667,  33.72      ,\n",
       "        35.68      ,  26.55333333,  21.23333333,  24.48      ,\n",
       "        18.03333333,  18.6       ,  15.80666667,  19.73333333,\n",
       "        21.42666667,  19.88      ,  18.86666667,  19.86      ,\n",
       "        22.7       ,  18.38      ,  19.00666667,  18.42      ,\n",
       "        14.13333333,  19.77333333,  15.36      ,  14.83333333,\n",
       "        17.18666667,  14.22666667,  17.16666667,  15.3       ,\n",
       "        18.86666667,  21.2       ,  14.36666667,  16.54      ,\n",
       "        14.86      ,  13.36666667,  13.48      ,  19.5       ,\n",
       "        20.56      ,  21.08666667,  22.48      ,  29.75333333,\n",
       "        34.55333333,  28.12      ,  25.16666667,  24.88666667,\n",
       "        21.67333333,  19.48666667,  20.05333333,  17.37333333,\n",
       "        16.15333333,  18.64      ,  20.18666667,  21.30666667,\n",
       "        24.42      ,  22.18      ,  18.98666667,  34.86      ,\n",
       "        24.17333333,  30.98666667,  23.14      ,  19.65333333,\n",
       "        18.72666667,  17.56      ,  22.62      ,  25.04      ,\n",
       "        32.21333333,  23.86666667,  20.1       ,  21.30666667,\n",
       "        17.75333333,  20.61333333,  23.85333333,  20.62      ,\n",
       "        22.88666667,  23.60666667,  23.98666667,  22.3       ,\n",
       "        20.41333333,  21.02666667,  20.82666667,  20.56666667,\n",
       "        26.79333333,  24.10666667,  24.38      ,  22.92      ,\n",
       "        22.47333333,  26.72      ,  21.56666667,  22.00666667,\n",
       "        26.46      ,  28.63333333,  22.42666667,  22.43333333,\n",
       "        22.64      ,  24.99333333,  21.02      ,  28.07333333,\n",
       "        23.1       ,  40.13333333,  43.04666667,  32.31333333,\n",
       "        27.06      ,  26.53333333,  18.66666667,  19.64666667,\n",
       "        20.36666667,  19.42      ,  19.07333333,  20.66      ,\n",
       "        20.16      ,  18.77333333,  21.24666667,  23.54666667,\n",
       "        19.47333333,  18.93333333,  19.58      ,  18.48666667,\n",
       "        20.46666667,  19.44      ,  19.72666667,  19.54      ,\n",
       "        21.86      ,  21.11333333,  20.19333333,  17.33333333,\n",
       "        19.05333333,  21.05333333,  15.72666667,  15.52666667,\n",
       "        17.28      ,  13.98      ,  19.31333333,  20.16      ,\n",
       "        20.84      ,  18.30666667,  16.11333333,  17.57333333,\n",
       "        17.01333333,  18.3       ,  14.27333333,  16.18      ,\n",
       "        14.20666667,  14.83333333,  13.4       ,  15.        ,\n",
       "        12.38666667,  13.96666667,  15.20666667,  14.82      ,\n",
       "        16.36      ,  14.54      ,  20.26      ,  18.78666667,\n",
       "        17.20666667,  16.48666667,  16.26      ,  16.        ,\n",
       "        13.70666667,  35.99333333,  23.9       ,  23.92      ,\n",
       "        26.52666667,  49.13333333,  49.01333333,  49.44666667,\n",
       "        21.74      ,  22.95333333,  50.        ,  21.92666667,\n",
       "        23.8       ,  22.95333333,  17.54666667,  19.94      ,\n",
       "        23.23333333,  23.28666667,  20.71333333,  29.47333333,\n",
       "        22.28      ,  23.84666667,  28.86666667,  36.72666667,\n",
       "        41.54666667,  29.34      ,  37.26666667,  30.99333333,\n",
       "        25.88666667,  26.16      ,  47.84666667,  30.36      ,\n",
       "        28.57333333,  35.        ,  36.48666667,  30.16      ,\n",
       "        35.71333333,  30.30666667,  28.36      ,  48.78666667,\n",
       "        33.79333333,  31.26666667,  34.84666667,  33.82      ,\n",
       "        33.89333333,  23.74      ,  45.08      ,  48.49333333,\n",
       "        48.8       ,  22.17333333,  23.73333333,  21.06666667,\n",
       "        23.66666667,  18.56      ,  20.55333333,  19.41333333,\n",
       "        21.16      ,  25.93333333,  23.32666667,  23.44666667,\n",
       "        22.70666667,  27.00666667,  21.06      ,  22.86666667,\n",
       "        25.88666667,  19.98      ,  26.92666667,  29.02666667,\n",
       "        44.18666667,  45.83333333,  39.48666667,  31.08      ,\n",
       "        46.91333333,  30.74666667,  22.72666667,  32.05333333,\n",
       "        41.73333333,  45.65333333,  28.42      ,  22.62666667,\n",
       "        26.55333333,  31.92      ,  23.94      ,  24.62      ,\n",
       "        22.88      ,  20.52666667,  21.97333333,  23.76      ,\n",
       "        17.92666667,  17.80666667,  23.19333333,  20.06      ,\n",
       "        24.45333333,  25.58666667,  24.41333333,  24.5       ,\n",
       "        29.98666667,  41.16666667,  22.30666667,  20.62      ,\n",
       "        44.92666667,  46.28      ,  35.26      ,  30.9       ,\n",
       "        33.45333333,  43.52666667,  47.22      ,  31.35333333,\n",
       "        35.74666667,  22.53333333,  30.32      ,  47.98666667,\n",
       "        43.54666667,  20.54666667,  20.83333333,  25.33333333,\n",
       "        24.97333333,  38.62      ,  31.77333333,  31.42      ,\n",
       "        33.94      ,  32.57333333,  26.82666667,  33.91333333,\n",
       "        45.74666667,  34.64      ,  45.46      ,  48.3       ,\n",
       "        30.9       ,  22.36666667,  22.70666667,  23.38666667,\n",
       "        22.44666667,  25.        ,  29.89333333,  37.08      ,\n",
       "        28.04666667,  23.12      ,  21.72      ,  27.94      ,\n",
       "        26.92666667,  19.57333333,  22.68666667,  31.74      ,\n",
       "        26.71333333,  24.02      ,  24.90666667,  32.68666667,\n",
       "        35.21333333,  27.91333333,  33.52666667,  29.18      ,\n",
       "        24.58      ,  20.36      ,  18.05333333,  23.60666667,\n",
       "        19.72666667,  22.40666667,  24.18666667,  17.28      ,\n",
       "        19.1       ,  19.64      ,  22.61333333,  21.00666667,\n",
       "        23.69333333,  23.14666667,  20.80666667,  19.13333333,\n",
       "        24.60666667,  24.5       ,  23.22      ,  20.6       ,\n",
       "        19.93333333,  22.76      ,  20.27333333,  18.54      ,\n",
       "        19.86      ,  21.91333333,  21.74666667,  21.12666667,\n",
       "        19.40666667,  19.62      ,  20.66666667,  19.30666667,\n",
       "        19.04666667,  32.54      ,  19.76      ,  25.48      ,\n",
       "        30.21333333,  18.22666667,  17.72      ,  23.5       ,\n",
       "        25.30666667,  27.93333333,  22.73333333,  23.98      ,\n",
       "        19.38666667,  28.98      ,  18.59333333,  20.72      ,\n",
       "        17.1       ,  20.86666667,  21.50666667,  21.96      ,\n",
       "        26.2       ,  20.21333333,  21.17333333,  17.19333333,\n",
       "        30.16666667,  28.42      ,  21.28      ,  20.86666667,\n",
       "        50.        ,  50.        ,  44.24666667,  44.9       ,\n",
       "        44.32      ,  13.23333333,  13.21333333,  17.91333333,\n",
       "        13.12      ,  13.55333333,  11.31333333,  11.65333333,\n",
       "        12.25333333,  11.34666667,  12.33333333,  12.27333333,\n",
       "         8.33333333,   7.39333333,   9.        ,   7.90666667,\n",
       "         9.69333333,  11.96666667,  14.84666667,  21.12666667,\n",
       "         9.70666667,  13.85333333,  12.46666667,  13.8       ,\n",
       "        13.31333333,   9.74666667,   5.82      ,   8.26666667,\n",
       "         6.77333333,   8.28      ,  12.28666667,   8.60666667,\n",
       "         8.41333333,   7.71333333,  13.66666667,  29.82      ,\n",
       "        16.48666667,  23.52      ,  23.89333333,  16.4       ,\n",
       "        17.18666667,  16.16666667,   8.24      ,   7.88      ,\n",
       "         9.51333333,   9.83333333,   8.36      ,   9.3       ,\n",
       "        16.25333333,  15.14      ,  20.74666667,  13.39333333,\n",
       "        14.96      ,   8.28666667,  14.02666667,  13.26666667,\n",
       "        11.16666667,  10.68666667,  14.72666667,  14.74666667,\n",
       "        17.17333333,  14.82666667,  12.80666667,  12.42      ,\n",
       "        11.06      ,   8.5       ,   8.48666667,  12.17333333,\n",
       "        10.06666667,  15.98      ,  17.75333333,  16.3       ,\n",
       "        10.71333333,  11.74666667,  15.37333333,  14.41333333,\n",
       "        14.15333333,  13.36      ,  13.51333333,  15.54666667,\n",
       "        16.52666667,  19.35333333,  14.62666667,  14.16      ,\n",
       "        13.08666667,  14.64666667,  14.52      ,  19.66666667,\n",
       "        15.86666667,  18.1       ,  20.39333333,  21.09333333,\n",
       "        21.37333333,  19.91333333,  15.92666667,  17.43333333,\n",
       "        16.52666667,  20.1       ,  18.25333333,  20.67333333,\n",
       "        20.96      ,  27.58      ,  13.86666667,  13.65333333,\n",
       "        16.11333333,  12.49333333,  15.64      ,  21.11333333,\n",
       "        22.3       ,  28.00666667,  32.86      ,  21.55333333,\n",
       "        20.38      ,  21.06      ,  18.72666667,  20.93333333,\n",
       "        14.53333333,   7.80666667,   8.60666667,  14.02666667,\n",
       "        20.49333333,  21.33333333,  23.29333333,  21.66      ,\n",
       "        18.23333333,  18.98666667,  21.60666667,  18.84      ,\n",
       "        18.79333333,  24.04      ,  20.99333333,  23.86      ,\n",
       "        23.85333333,  14.40666667])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#随机森林回归树预测结果\n",
    "rgs.predict(boston_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,\n",
       "        18.9,  15. ,  18.9,  21.7,  20.4,  18.2,  19.9,  23.1,  17.5,\n",
       "        20.2,  18.2,  13.6,  19.6,  15.2,  14.5,  15.6,  13.9,  16.6,\n",
       "        14.8,  18.4,  21. ,  12.7,  14.5,  13.2,  13.1,  13.5,  18.9,\n",
       "        20. ,  21. ,  24.7,  30.8,  34.9,  26.6,  25.3,  24.7,  21.2,\n",
       "        19.3,  20. ,  16.6,  14.4,  19.4,  19.7,  20.5,  25. ,  23.4,\n",
       "        18.9,  35.4,  24.7,  31.6,  23.3,  19.6,  18.7,  16. ,  22.2,\n",
       "        25. ,  33. ,  23.5,  19.4,  22. ,  17.4,  20.9,  24.2,  21.7,\n",
       "        22.8,  23.4,  24.1,  21.4,  20. ,  20.8,  21.2,  20.3,  28. ,\n",
       "        23.9,  24.8,  22.9,  23.9,  26.6,  22.5,  22.2,  23.6,  28.7,\n",
       "        22.6,  22. ,  22.9,  25. ,  20.6,  28.4,  21.4,  38.7,  43.8,\n",
       "        33.2,  27.5,  26.5,  18.6,  19.3,  20.1,  19.5,  19.5,  20.4,\n",
       "        19.8,  19.4,  21.7,  22.8,  18.8,  18.7,  18.5,  18.3,  21.2,\n",
       "        19.2,  20.4,  19.3,  22. ,  20.3,  20.5,  17.3,  18.8,  21.4,\n",
       "        15.7,  16.2,  18. ,  14.3,  19.2,  19.6,  23. ,  18.4,  15.6,\n",
       "        18.1,  17.4,  17.1,  13.3,  17.8,  14. ,  14.4,  13.4,  15.6,\n",
       "        11.8,  13.8,  15.6,  14.6,  17.8,  15.4,  21.5,  19.6,  15.3,\n",
       "        19.4,  17. ,  15.6,  13.1,  41.3,  24.3,  23.3,  27. ,  50. ,\n",
       "        50. ,  50. ,  22.7,  25. ,  50. ,  23.8,  23.8,  22.3,  17.4,\n",
       "        19.1,  23.1,  23.6,  22.6,  29.4,  23.2,  24.6,  29.9,  37.2,\n",
       "        39.8,  36.2,  37.9,  32.5,  26.4,  29.6,  50. ,  32. ,  29.8,\n",
       "        34.9,  37. ,  30.5,  36.4,  31.1,  29.1,  50. ,  33.3,  30.3,\n",
       "        34.6,  34.9,  32.9,  24.1,  42.3,  48.5,  50. ,  22.6,  24.4,\n",
       "        22.5,  24.4,  20. ,  21.7,  19.3,  22.4,  28.1,  23.7,  25. ,\n",
       "        23.3,  28.7,  21.5,  23. ,  26.7,  21.7,  27.5,  30.1,  44.8,\n",
       "        50. ,  37.6,  31.6,  46.7,  31.5,  24.3,  31.7,  41.7,  48.3,\n",
       "        29. ,  24. ,  25.1,  31.5,  23.7,  23.3,  22. ,  20.1,  22.2,\n",
       "        23.7,  17.6,  18.5,  24.3,  20.5,  24.5,  26.2,  24.4,  24.8,\n",
       "        29.6,  42.8,  21.9,  20.9,  44. ,  50. ,  36. ,  30.1,  33.8,\n",
       "        43.1,  48.8,  31. ,  36.5,  22.8,  30.7,  50. ,  43.5,  20.7,\n",
       "        21.1,  25.2,  24.4,  35.2,  32.4,  32. ,  33.2,  33.1,  29.1,\n",
       "        35.1,  45.4,  35.4,  46. ,  50. ,  32.2,  22. ,  20.1,  23.2,\n",
       "        22.3,  24.8,  28.5,  37.3,  27.9,  23.9,  21.7,  28.6,  27.1,\n",
       "        20.3,  22.5,  29. ,  24.8,  22. ,  26.4,  33.1,  36.1,  28.4,\n",
       "        33.4,  28.2,  22.8,  20.3,  16.1,  22.1,  19.4,  21.6,  23.8,\n",
       "        16.2,  17.8,  19.8,  23.1,  21. ,  23.8,  23.1,  20.4,  18.5,\n",
       "        25. ,  24.6,  23. ,  22.2,  19.3,  22.6,  19.8,  17.1,  19.4,\n",
       "        22.2,  20.7,  21.1,  19.5,  18.5,  20.6,  19. ,  18.7,  32.7,\n",
       "        16.5,  23.9,  31.2,  17.5,  17.2,  23.1,  24.5,  26.6,  22.9,\n",
       "        24.1,  18.6,  30.1,  18.2,  20.6,  17.8,  21.7,  22.7,  22.6,\n",
       "        25. ,  19.9,  20.8,  16.8,  21.9,  27.5,  21.9,  23.1,  50. ,\n",
       "        50. ,  50. ,  50. ,  50. ,  13.8,  13.8,  15. ,  13.9,  13.3,\n",
       "        13.1,  10.2,  10.4,  10.9,  11.3,  12.3,   8.8,   7.2,  10.5,\n",
       "         7.4,  10.2,  11.5,  15.1,  23.2,   9.7,  13.8,  12.7,  13.1,\n",
       "        12.5,   8.5,   5. ,   6.3,   5.6,   7.2,  12.1,   8.3,   8.5,\n",
       "         5. ,  11.9,  27.9,  17.2,  27.5,  15. ,  17.2,  17.9,  16.3,\n",
       "         7. ,   7.2,   7.5,  10.4,   8.8,   8.4,  16.7,  14.2,  20.8,\n",
       "        13.4,  11.7,   8.3,  10.2,  10.9,  11. ,   9.5,  14.5,  14.1,\n",
       "        16.1,  14.3,  11.7,  13.4,   9.6,   8.7,   8.4,  12.8,  10.5,\n",
       "        17.1,  18.4,  15.4,  10.8,  11.8,  14.9,  12.6,  14.1,  13. ,\n",
       "        13.4,  15.2,  16.1,  17.8,  14.9,  14.1,  12.7,  13.5,  14.9,\n",
       "        20. ,  16.4,  17.7,  19.5,  20.2,  21.4,  19.9,  19. ,  19.1,\n",
       "        19.1,  20.1,  19.9,  19.6,  23.2,  29.8,  13.8,  13.3,  16.7,\n",
       "        12. ,  14.6,  21.4,  23. ,  23.7,  25. ,  21.8,  20.6,  21.2,\n",
       "        19.1,  20.6,  15.2,   7. ,   8.1,  13.6,  20.1,  21.8,  24.5,\n",
       "        23.1,  19.7,  18.3,  21.2,  17.5,  16.8,  22.4,  20.6,  23.9,\n",
       "        22. ,  11.9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#回归树建模\n",
    "from sklearn import tree\n",
    "#构建回归树模型\n",
    "rgs2 = tree.DecisionTreeRegressor()\n",
    "rgs2.fit(boston_features, boston_target)\n",
    "#回归树模型的预测结果\n",
    "rgs2.predict(boston_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "补充知识点---随机森林回归树模型各参数的含义：\n",
    "函数：class sklearn.ensemble.RandomForestRegressor(n_estimators=10, criterion=’mse’, max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "n_jobs=1, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "1.n_estimators：森林中数的个数。 \n",
    "这个属性是典型的模型表现与模型效率成反比的影响因子，即便如此，你还是应该尽可能提高这个数字，以让你的模型更准确更稳定。 \n",
    "2.criterion ：度量分裂的标准。可选值：“mse”，均方差（mean squared error）；“mae”，平均绝对值误差（mean absolute error） \n",
    "3.max_features ：寻找最佳分裂点时考虑的属性数目。可选值，int（具体的数目），float（数目的百分比），string（“auto”， “sqrt”，“log2”）. \n",
    "这一属性是对单个树来设置的，通常来讲，这个值越大单棵树可以考虑的属性越多，则模型的表现就越好。但是这也不是肯定的，不过有一点是肯定的，增加这个值会导致算法运行速度变慢，所以需要我们考虑去达到一个平衡。 \n",
    "4.max_depth : integer或者None。数的最大深度，如果None，节点扩展直到所有叶子是纯的或者所有叶子节点包含的样例数小于min_samples_split  \n",
    "5.min_samples_split ： 分裂内部节点需要的最少样例数。int(具体数目),float(数目的百分比) \n",
    "6.min_samples_leaf ：叶子节点上应有的最少样例数。int(具体数目),float(数目的百分比)。 \n",
    "更少的节点数使得模型更容易遭受noise data的影响，我通常设置这个值大于50，但是你需要寻找最适合你的数值。 \n",
    "7.min_weight_fraction_leaf ： \n",
    "8.max_leaf_nodes ：以”最优优先方式”(best-first fashion),最优节点定义为:纯度的相对减少.如果None则不限制叶子节点个数;[float] \n",
    "9.min_impurity_split : 树增长提前结束的阈值.对于当前节点,大于这个阈值将分裂,否则就看做叶子节点; [float] \n",
    "10.min_impurity_decrease ：一个阈值,表示一个节点分裂的条件是:如果这次分裂纯度的减少大于等于这这个值. \n",
    "11.bootstrap ：构建数是不是采用有放回样本的方式(bootstrap samples);  [True/False] \n",
    "12.oob_score ：交叉验证相关的属性。 \n",
    "13.n_jobs ：设定fit和predict阶段并列执行的任务个数,如果设置为-1表示并行执行的任务数等于计算级核数; [integer, optional (default=1)] \n",
    "14.random_state ：如果是int数值表示它就是随机数产生器的种子.如果指定RandomState实例,它就是随机产生器的种子.如果是None,随机数产生器是np.random所用的RandomState实例;  [int, RandomState instance or None, optional (default=None)] \n",
    "15.verbose ：控制构建数过程的冗长度;  [int, optional (default=0)] \n",
    "16.warm_start ：当设置为True,重新使用之前的结构去拟合样例并且加入更多的估计器(estimators,在这里就是随机树)到组合器中; [True/False] \n",
    "17.class_weight: “banlanced”模式是根据y标签值自动调整权值与输入数据的类频率成反比,计算公式是:n_samples / (n_classes  np.bincount(y)).“balanced_subsample”模式的与”balanced模式相同,只不过在每一次树增长过程中权值的计算是根据有放回样本的.\n",
    "\n",
    "补充随机森林的练习：\n",
    "参考链接1：https://blog.csdn.net/u012559520/article/details/77336098\n",
    "参考链接2：https://blog.csdn.net/qq_16633405/article/details/61200502\n",
    "参考链接3：https://www.programcreek.com/python/example/86074/sklearn.ensemble.RandomForestRegressor\n",
    "参考链接4：https://blog.csdn.net/luoganttcc/article/details/80618336\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.通过本案例掌握回归树的基本原理；\n",
    "\n",
    "2.通过本案例加深对随机森林算法的理解；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作者：寒小阳、助教：haoshuai714\n",
    "\n",
    "本文档版权归稀牛学院所有"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "![](http://osloyi5le.bkt.clouddn.com/%E7%A8%80%E7%89%9B%20x%20%E7%BD%91%E6%98%93.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
